{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b73d77",
   "metadata": {},
   "source": [
    "# Integrantes\n",
    "* ### David Herrera\n",
    "* ### Estid Lozano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7387b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebddc779",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd62f2d9",
   "metadata": {},
   "source": [
    "**1)** Suppose that we have a linearly separable dataset with instances for both positive and negative classes, and we have an optimal hyperplane h computed by the SVM (for linearly separable problems). Show that there is at least one support vector for either of the two classes for h."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9079c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If **h** is optimal, its weight is calculated using:\n",
    "\n",
    "$ \\LARGE h^* = argmax_h({{\\delta^*}_h}) = argmax_{w,b}(\\frac{1}{\\lvert w \\rvert})$\n",
    "\n",
    "and using lagrange we get:\n",
    "\n",
    "$\\LARGE \\min \\boldsymbol{L} =\\frac{1}{2} {\\lvert w \\rvert}^2- \\sum_{i=1}^{n} \\alpha_{i}(y_i(w^t x_i+b)-1) $\n",
    "\n",
    "and we find:\n",
    "\n",
    "$ \\LARGE \\frac{\\delta}{\\delta b}L=\\sum_{i=1}^{n} \\alpha_i y_i = 0$ and $\\LARGE W = \\sum_{i=1}^{n} \\alpha_i y_i x_i  $\n",
    "\n",
    "* the points with $\\alpha_i= 0$ are not support vectors and thus do not play a role in determining w\n",
    "* making at least two points with different classes are in D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb3f771",
   "metadata": {},
   "source": [
    "**2)** Show that\n",
    "\n",
    "$ \\Large \\frac{\\delta (w^2 / 2)}{\\delta w_{k}}= w_{k} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d514438",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "w=\n",
    "\\begin{pmatrix}\n",
    "   w_{1} \\\\\n",
    "   w_{2} \\\\\n",
    "   w_{3} \\\\\n",
    "   ...   \\\\\n",
    "   w_{n} \\\\\n",
    "\\end{pmatrix} \\\\\n",
    "\\Large \\rightarrow \\\\\n",
    "\\Large\n",
    "{w^2 \\over 2 }= \n",
    "\\begin{pmatrix}\n",
    "   w_{1}^2 \\over 2 \\\\\n",
    "   w_{2}^2 \\over 2 \\\\\n",
    "   w_{3}^2 \\over 2 \\\\\n",
    "   ... \\\\\n",
    "   w_{n}^2 \\over 2 \\\\\n",
    "\\end{pmatrix} \\\\\n",
    "\\Large \\rightarrow \\\\\n",
    "\\Large\n",
    "{\\delta (w^2 / 2) \\over \\delta w_{k}}= \n",
    "\\begin{pmatrix}\n",
    "   \\delta (w_{1}^2 / 2 ) \\over \\delta w_{k} \\\\\n",
    "   \\delta (w_{2}^2 / 2 ) \\over \\delta w_{k} \\\\\n",
    "   \\delta (w_{3}^2 / 2 ) \\over \\delta w_{k} \\\\\n",
    "   ... \\\\\n",
    "   \\delta (w_{k}^2 / 2 ) \\over \\delta w_{k} \\\\\n",
    "   ... \\\\\n",
    "   \\delta (w_{n}^2 / 2 ) \\over \\delta w_{k}\\\\\n",
    "\\end{pmatrix} \\\\\n",
    "\\Large \\rightarrow \\\\\n",
    "\\Large\n",
    "{\\delta (w^2 / 2) \\over \\delta w_{k}}= \n",
    "\\begin{pmatrix}\n",
    "   0 \\\\\n",
    "   0 \\\\\n",
    "   0 \\\\\n",
    "   ... \\\\\n",
    "   w_{k}\\\\\n",
    "   ... \\\\\n",
    "   0\\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "w_{k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6731b365",
   "metadata": {},
   "source": [
    "**3)** When solving the optimization problem in Eq. (21.9) directly with a gradient descent algorithm, we need to fix two problems. Describe how you would do this:\n",
    "\n",
    "**a)** it seems that there is no obvious initial solution. Provide an algorithm that will generate a feasible solution for w, b.\n",
    "1) set 1 in the first position the w vector  and the remainder in 0\n",
    "\n",
    "2) set b as the average of the first attribute of all points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f720a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1.12, 2.13], [0.56, 1.42], [2.87, 0.99], [2.17, 0.43]])\n",
    "Y = np.array([-1, -1, 1, 1])\n",
    "def get_b_w(_X, _Y):\n",
    "    # find mid point of each class\n",
    "    mid1 = np.average([x for x, y in zip(_X, _Y) if y == 1], axis=0)\n",
    "    mid2 = np.average([x for x, y in zip(_X, _Y) if y == -1], axis=0)\n",
    "    # find mid point\n",
    "    mid = np.add(mid1, mid2)/2\n",
    "    # vector between mid points\n",
    "    u = mid2-mid1\n",
    "    # perpendicular vector\n",
    "    v = np.array([-u[1], u[0]])\n",
    "    # weigths\n",
    "    w1 = v[1]  # w1=1\n",
    "    w2 = -v[0]  # w2=-1\n",
    "    b = (-w1*mid[0])-(w2*mid[1])\n",
    "    w = np.array([w1, w2])\n",
    "    return b, w\n",
    "b, w = get_b_w(X, Y)\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc7e0e",
   "metadata": {},
   "source": [
    "**b)** On one hand, the gradient of the objective function for b is 0, so we would never adjust it with gradient descent. On the other hand, very small entries in w might violate the linear contraints for any b. Describe an algorithm that determines whether, for a given w, there is a b so that the constraints are satisfied, and that returns such a b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8464bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkW(_w,_X,_Y):\n",
    "    return sum([y1-np.dot(_w,x1) for x1,y1 in zip(_X,_Y)])/len(_X)\n",
    "print(checkW(w,X,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09380b1",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Implement a learner SVM (no need for probabilities) that can perform binary classification by determining a maximum margin hyperplane.\n",
    "\n",
    "Train the SVM using a gradient descent algorithm solving the optimization problem in Eq.\n",
    "(21.9). Use your algorithms from exercise 1 (3) to use an initial solution, which you then\n",
    "update using the update rule\n",
    "wt+1 ← wt − ηwt\n",
    "\n",
    "Here, η is a step size, which is a parameter of the algorithm. Make sure to update b so that\n",
    "you maintain a valid solution.\n",
    "\n",
    "You can use a step size η = 0.1 and ε = 0.01 by default; you can also make your SVM\n",
    "configurable in these parameters.\n",
    "\n",
    "Make sure that the weight vector (including the bias weight in the first place) is stored in the\n",
    "variable w, so that it can be accessed after training.\n",
    "\n",
    "Implement the predict method in order to obtain predictions from your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7ce731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM():\n",
    "    def __init__(self, _n=0.1, _e=0.01):\n",
    "        self._e = _e\n",
    "        self._n = _n\n",
    "\n",
    "    def train(self, _X, _Y):\n",
    "        b, w = get_b_w(_X, _Y)\n",
    "        w = np.array([b, w[0], w[1]])\n",
    "        W = [w]\n",
    "        t = 0\n",
    "        while(True):\n",
    "            wCopy = W[-1]\n",
    "            for x, y in zip(_X, _Y):\n",
    "                x = np.insert(x, 0, 1, axis=0)\n",
    "                gradient = None\n",
    "                if 1 - y * (np.dot(wCopy, x)) <= 0:\n",
    "                    gradient = wCopy\n",
    "                else:\n",
    "                    gradient = wCopy - x*y # sum([y1 * x1 for x1, y1 in zip(_X, _Y)])\n",
    "                wCopy = wCopy - ((1/(t+1000)) * gradient)\n",
    "                # wCopy[0] = checkW(wCopy[1:], _X, _Y)\n",
    "            t += 1\n",
    "            W.append(wCopy)\n",
    "            if np.linalg.norm(W[t]-W[t-1]) <= self._e:\n",
    "                break\n",
    "        W[-1][0] = checkW(W[-1][1:], _X, _Y)\n",
    "        self.w = W[-1]\n",
    "\n",
    "    def predict(self, p):\n",
    "        res = []\n",
    "        p = np.insert(p, 0, 1, axis=1)\n",
    "        for pr in p:\n",
    "            res.append(np.sign(np.dot(self.w, pr)))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d0f5f-1e7b-4790-bc8a-d1f836f8e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris.csv')\n",
    "df['species'] = df['species'].apply(lambda s: 1 if s == 'setosa' else -1)\n",
    "X = df.iloc[:, :2].to_numpy()\n",
    "Y = df.iloc[:, -1].to_numpy()\n",
    "\n",
    "model = SVM()\n",
    "model.train(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efcf7c7",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "Run the script in the template to obtain plots for all SVM models\n",
    "obtained for attribute pairs of the Iris dataset (binarized by merging versicolor and virginica).\n",
    "Is your SVM able to find a perfect separation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c551000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHyperplane(X, y, w, ax=None):\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # scatter points\n",
    "    classes = list(np.unique(y))\n",
    "    X1 = X[y == classes[0]]\n",
    "    X2 = X[y == classes[1]]\n",
    "    ax.scatter(X1[:,0], X1[:,1])\n",
    "    ax.scatter(X2[:,0], X2[:,1])\n",
    "    \n",
    "    # compute slope and intercept representing the hyperplane\n",
    "    random_vec = np.random.rand(2)\n",
    "    slope = -w[1] / w[2]\n",
    "    intercept = -w[0]/w[2]\n",
    "    a2_min, a2_max = min(X[:,1]), max(X[:,1])\n",
    "    q1 = (a2_max - intercept) / slope\n",
    "    q2 = (a2_min - intercept) / slope\n",
    "    x_from = max(min(X[:,0]), min(q1, q2))\n",
    "    x_to = min(max(X[:,0]), max(q1, q2))\n",
    "    hp = lambda x: x * slope + intercept\n",
    "    \n",
    "    # plot the hyperplane\n",
    "    domain = np.linspace(x_from, x_to, 100)\n",
    "    ax.plot(domain, hp(domain), color=\"black\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5c705f-1dd5-4daf-864e-52ec9f81e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIris = pd.read_csv(\"iris.csv\")\n",
    "\n",
    "fig, ax = plt.subplots(6, 1, figsize=(12, 42))\n",
    "\n",
    "y = dfIris.replace(\"virginica\", \"versicolor\").values[:,4]\n",
    "y = np.where(y == 'setosa', 1, -1)\n",
    "\n",
    "for j, combo in enumerate(it.combinations(range(4), 2)):\n",
    "    X = dfIris.values[:,combo].astype(float)\n",
    "    svc = SVM()\n",
    "    svc.train(X, y)\n",
    "    plotHyperplane(X, y, svc.w, ax[j])\n",
    "    # ax[j].set_title(\"\\nSlack: \" + str(np.round(np.sum(np.maximum(0, svc.slacks)), 2)))\n",
    "    predictions = svc.predict(X)\n",
    "    ax[j].set_xlabel(dfIris.columns[combo[0]])\n",
    "    ax[j].set_ylabel(dfIris.columns[combo[1]])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0852739e-8f1f-457c-a874-44c28acc4022",
   "metadata": {},
   "source": [
    "**Answer:** Yes. Although our SVM does not always find a perfect separation; for example, in the case of the sepal width with the sepal length. In several cases such as the petal length with sepal width (also with the sepal length) or the petal width vs the petal length, it finds a perfect separation."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
