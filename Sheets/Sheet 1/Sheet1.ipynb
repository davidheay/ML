{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Sheet 1 \n",
                "* David Herrera"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Exercise 1"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "1) For the following two situations, decide whether they are supervised learning problems. Explain your answer by describing what the instance space X , the label space Y, and the true relationship f are: \n",
                "\n",
                "**A)** fitness gym analyzes the heartbeat of the clients during their exercise. They want\n",
                "to automatically detect when the heart activity of a client is irregular in order to\n",
                "check his status and be prepared for calling an ambulance.\n",
                "\n",
                "This is not a supervised learning problem because we are not given a label that indicates when the heart activity is irregular.\n",
                "\n",
                "X: heart activity with features: heart rate,\n",
                "\n",
                "Y: {yes, no\\}. binary classification\n",
                "\n",
                "f: Is a probability mass function that associates a probability of iregular heart activity to an instance.\n",
                "\n",
                "**B)**  You are running a business and your employees use vehicles of your company. You want to check whether somebody is stealing gas while tanking the vehicles. To this end, all tanking transactions are registered in a database with the tanked vehicle, the amount of tanked gas, and the mileage of the vehicle. From this data you want\n",
                "to learn a model that predicts for a new transaction whether it is suspicious for fraud.\n",
                "\n",
                "This is not a supervised learning problem because we are not given a label that indicates fraud.\n",
                "\n",
                "X: transactions in database with features: the tanked vehicle, the amount of tanked gas, and the mileage of the vehicle\n",
                "\n",
                "Y: {yes, no\\}. binary classification\n",
                "\n",
                "f: Is a probability mass function that associates a probability of fraud to an instance."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "2) Suppose that you have fixed a particular hypothesis h for your learning problem and\n",
                "say that you want to estimate the out-of-sample-error up to a precision of 0.01 with a\n",
                "probability of at least 90%. How many instances do you need in your test data to obtain\n",
                "such a certainty according to the Hoeffding inequality?\n",
                "\n",
                "<img src=\"P2.png\" width=\"400\" style=\"display: block;margin-left: auto;margin-right: auto;width: 50%;\"> \n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "3) Now suppose that you have not fixed your hypothesis before but only made a preselection of k potential hypothesises. How many instances do you now need in your\n",
                "validation fold to have the same certainty about the generalization error when you are\n",
                "allowed to pick the hypothesis with the best score in the validation fold?\n",
                "\n",
                "<img src=\"P3.png\" width=\"500\" style=\"display: block;margin-left: auto;margin-right: auto;width: 50%;\"> "
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Exercise 2"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "4) Implement the kNN algorithm shown in the lecture to make predictions. Write a class\n",
                "in Python called NearestNeighbors with two functions train(X, y) and predict(X).\n",
                "Here, X is a 2D numpy array (a matrix with n lines for instances and d columns for\n",
                "attributes), and y is a n-vector with the labels for the given instances. The function\n",
                "train memorizes the data (does not return anything), and predict returns a vector of\n",
                "same length as the input matrix with the predicted label for each given instance (you may\n",
                "want to check the argsort function of numpy for this). The class NearestNeighbors\n",
                "might have a parameter k that defines the number of parameters considered here"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "# imports\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import math"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "source": [
                "# def class\n",
                "class NearestNeighbors:\n",
                "    def __init__(self, K):\n",
                "        self.K = K\n",
                "\n",
                "    def train(self, X, Y):\n",
                "        self.X = X\n",
                "        self.Y = Y\n",
                "\n",
                "    def predict(self, Y):\n",
                "        # answers array\n",
                "        predicted_labels = []\n",
                "        # for each point to predict\n",
                "        for y in Y:\n",
                "            distances = []\n",
                "            # add distance and label  to array\n",
                "            for index in range(len(self.X)):\n",
                "                point = self.X[index]\n",
                "                label = self.Y[index]\n",
                "                distances.append([np.linalg.norm(point-y), label])\n",
                "            # sort array by distance\n",
                "            distances = sorted(distances, key=lambda x: x[0])\n",
                "            # get K nearest neighbor distance\n",
                "            r = distances[self.K][0]\n",
                "            # filter points <= r\n",
                "            distances = np.array(list(filter(lambda x: x[0] <= r, distances)))\n",
                "            # count labels occurence\n",
                "            labels, counts = np.unique(distances[:, 1], return_counts=True)\n",
                "            # get index of max label\n",
                "            indMaxLabel = counts.argmax()\n",
                "            # add label to predict array\n",
                "            predicted_labels.append(labels[indMaxLabel])\n",
                "        return predicted_labels\n",
                "\n",
                "# Read dataset\n",
                "df = pd.read_csv('iris.csv')\n",
                "# get first 4 rows\n",
                "X = df.iloc[:, :4].to_numpy()\n",
                "# get labels\n",
                "Y = df.iloc[:, 4].to_numpy()\n",
                "# create model\n",
                "model = NearestNeighbors(5)\n",
                "model.train(X, Y)\n",
                "arrayToPredict = np.array(\n",
                "    [[4.9, 3.0, 1.4, 0.2], [5.5, 2.3, 4, 1.3], [7.2, 3.6, 6.1, 2.5]]\n",
                ")\n",
                "print(model.predict(arrayToPredict))\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['setosa', 'versicolor', 'virginica']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "5) Implement a function cross_validate(learner, X, y, train_size, repeats) that\n",
                "takes a learner object and builds repeats many random splits of the given data, each one\n",
                "with a relative portion of train_size training instances (for example 0.7 to use 70% of\n",
                "the data for training in each iteration) and the rest for validation. In each main loop of\n",
                "the validation, a random split of the data is built (you may want to use random.sample to\n",
                "get a random subset of indices for the chosen instances) and the learner is trained on the\n",
                "training portion and then must make predictions on the remaining data. Compare the\n",
                "predictions to the true labels of the remaining data and count the number of mistakes.\n",
                "For each repetition, memorize the mean number of false predictions, called the error\n",
                "rate. Then finally return the mean error rate of the folds."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# imports\n",
                "import random"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def cross_validate(learner, X, Y, train_size, repeats):\n",
                "    # init error rate\n",
                "    errorRate = 0\n",
                "    for repeat in range(repeats):\n",
                "        # get sample indexes  to train\n",
                "        trainIndexes = random.sample(range(len(X)), int(len(X)*train_size))\n",
                "        # get sample indexes  to test\n",
                "        testIndexes = [val for val in range(len(X)) if val not in trainIndexes]\n",
                "        # get the sample of instances to train\n",
                "        trainX = X.iloc[trainIndexes].to_numpy()\n",
                "        # get the sample of labels to train\n",
                "        trainY = Y.iloc[trainIndexes].to_numpy()\n",
                "        # get the sample of instances to test\n",
                "        testX = X.iloc[testIndexes].to_numpy()\n",
                "        # get the sample of labels to test\n",
                "        testY = Y.iloc[testIndexes].to_numpy()\n",
                "        # train the learner with the samples\n",
                "        learner.train(trainX, trainY)\n",
                "        # predict with test instances sample \n",
                "        # compare to test learner sample\n",
                "        # subtract the amount from the total test labels with the comparison\n",
                "        errorRate = errorRate + (len(testY)-sum(learner.predict(testX) == testY))/len(testY)\n",
                "    return errorRate/repeats\n",
                "# Read dataset\n",
                "df = pd.read_csv('iris.csv')\n",
                "# get first 4 rows\n",
                "X = df.iloc[:, :4]\n",
                "# get labels\n",
                "Y = df.iloc[:, 4]\n",
                "# test\n",
                "cross_validate(NearestNeighbors(5), X, Y, 0.7, 10)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "6) Load the iris dataset (you can use pandas.read_csv) and create objects X and y from\n",
                "it, using the class column for y. Compute and report the average error rate of the kNN\n",
                "classifier for 10 repetitions and a training size of 70%.\n",
                "Now compute the error for different training set sizes of 5, 10, 15, ..., 100 training\n",
                "instances (choose the training portion appropriately). Collect the results and plot a\n",
                "learning curve. How many training examples are necessary on average to obtain a error\n",
                "rate lower than 10%?"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# imports\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Read dataset\n",
                "df = pd.read_csv('iris.csv')\n",
                "# get first 4 rows\n",
                "X = df.iloc[:, :4]\n",
                "# get labels\n",
                "Y = df.iloc[:, 4]\n",
                "# test\n",
                "errorMean = cross_validate(NearestNeighbors(5), X, Y, 0.7, 10)\n",
                "print(\"Error mean with 10 repetitions and a training size of 70%. = \",errorMean)\n",
                "# array to plot errors\n",
                "errors = []\n",
                "# array from 5 to 105\n",
                "temp =  range(5,105,5)\n",
                "for size in tqdm(temp):\n",
                "    # get percentage to get the wanted size\n",
                "    percentage=size/len(X)\n",
                "    # add error to array\n",
                "    errors.append(cross_validate(NearestNeighbors(4), X, Y, percentage, 100))\n",
                "# plot learning curve\n",
                "plt.plot(temp, errors)\n",
                "plt.xlabel(\"training set size\")\n",
                "plt.ylabel(\"% error\")\n",
                "print(\"to obtain a error rate lower than 10% are necessary at least 20 training examples\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "7) In this exercise, we only consider the first two attributes (columns) of the iris dataset\n",
                "(and the class column). Now train the kNN classifier on all the data X from the iris\n",
                "dataset using the first two attributes.\n",
                "Now create discretizations of the domains U = [4, 8] and V = [2, 5] such that each\n",
                "domain has 100 values (see linspace of numpy). We can consider U and V as being\n",
                "lists of length 100 each; denote as Ui or Vi the i-the element of each list.\n",
                "Create a 100 × 100 matrix Z in which Zi,j is the prediction of the kNN classifier on the\n",
                "instance xi,j = (Ui\n",
                ", Vj ).\n",
                "Now create 2 plots. In the first, create a scatter plot in which you show all the data of the\n",
                "iris set, each class in a different color. In the second, create an image map of the matrix\n",
                "Z (use the imshow function of matplotlib). For the latter, replace the string-valued\n",
                "predictions by the values 0, 1, 2.\n",
                "Do you think that kNN manages to do a good separation of the classes?"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Read dataset\n",
                "df = pd.read_csv('iris.csv')\n",
                "# get first 4 rows\n",
                "X = df.iloc[:, :2].to_numpy()\n",
                "# get labels\n",
                "Y = df.iloc[:, 4].to_numpy()\n",
                "# def num of points\n",
                "length = 100\n",
                "# create domains\n",
                "U = np.linspace(4.0, 8.0, num=length)\n",
                "V = np.linspace(2.0, 5.0, num=length)\n",
                "# create Z\n",
                "Z = [['' for i in range(length)] for j in range(length)]\n",
                "# create model\n",
                "model = NearestNeighbors(5)\n",
                "model.train(X, Y)\n",
                "# dictionary to convert to 0,1,2\n",
                "dicti = ['setosa', 'versicolor', 'virginica']\n",
                "# fill Z\n",
                "for i in tqdm(range(length)):\n",
                "    for j in range(length):\n",
                "        Z[i][j] = dicti.index(model.predict([[U[i], V[j]]])[0])\n",
                "\n",
                "# plot graphs\n",
                "colors = {'setosa': 'purple', 'versicolor': 'gray', 'virginica': 'yellow'}\n",
                "fig = plt.figure()\n",
                "fig.set_size_inches(20, 10)\n",
                "ax = fig.add_subplot(1, 2, 1)\n",
                "ax.set_xlabel(df.columns[0])\n",
                "ax.set_ylabel(df.columns[1])\n",
                "ax.scatter(X[:, 0], X[:, 1], edgecolor='k', c=df[\"species\"].map(colors))\n",
                "# ax = fig.add_subplot(1, 2, 2)\n",
                "ax.imshow(np.array(Z).transpose(),extent=(4,8,2,5),origin='lower')\n",
                "ax.grid(b=None)\n",
                "ax.grid()\n"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit"
        },
        "interpreter": {
            "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}