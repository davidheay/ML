{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Integrantes\n",
    "* ### David Herrera\n",
    "* ### Estid Lozano"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "source": [
    "# Imports\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openml\n",
    "import pandas as pd\n",
    "import scipy.stats"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1\n",
    "### 1.1 Write a probabilistic learner LDA that builds models for binary classification via the linear discriminant analysis. Prediction should be made assuming a (1-dimensional) normal distribution for each class with means and variances according to the built model. When returning probabilities, normalize the densities assigned to each class so that the vector sums up to 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "source": [
    "class LDA():\n",
    "    def train(self, _X, _Y):\n",
    "        data = {}\n",
    "        for x, y in zip(_X, _Y):\n",
    "            if y not in data.keys():\n",
    "                data[y] = []\n",
    "            data[y].append(x)\n",
    "        self.data = data.copy()\n",
    "        mean = {}\n",
    "        centerData = data\n",
    "        S = {}\n",
    "\n",
    "        for key in data:\n",
    "            mean[key] = np.mean(data[key], axis=0)\n",
    "            centerData[key] -= mean[key].T\n",
    "            S[key] = np.dot(centerData[key].T, centerData[key])\n",
    "\n",
    "        S = np.sum(list(S.values()), axis=0)\n",
    "        S_1 = np.linalg.inv(S)\n",
    "\n",
    "        self.means = mean\n",
    "        means = list(mean.values())\n",
    "        diffMean = means[0]-means[1]\n",
    "\n",
    "        B = np.outer(diffMean, diffMean)\n",
    "        S_1B = np.dot(S_1, B)\n",
    "        w = None\n",
    "        if np.linalg.det(S):\n",
    "            w = np.dot(S_1, diffMean)\n",
    "            w = w/np.linalg.norm(w)\n",
    "        else:\n",
    "            values, vectors = np.linalg.eig(S_1B)\n",
    "            w = vectors[:, values.argmax()]\n",
    "        self.w = w\n",
    "\n",
    "    def predict(self, _X):\n",
    "        res = []\n",
    "        for x in _X:\n",
    "            resTemp = []\n",
    "            x = np.dot(self.w, x)\n",
    "            for key in self.data:\n",
    "                projectedPoints = np.array([np.dot(self.w, x) for x in self.data[key]])\n",
    "                # projectedMean=np.mean(projectedPoints)\n",
    "                projectedMean = np.dot(self.w, self.means[key])\n",
    "                variance = np.var(projectedPoints)\n",
    "                normal = scipy.stats.norm(projectedMean, variance).pdf(x)\n",
    "                resTemp.append(normal)\n",
    "            resTemp = resTemp/np.linalg.norm(resTemp)\n",
    "            res.append(list(self.data.keys())[resTemp.argmax()])\n",
    "        return res\n",
    "\n",
    "\n",
    "df = pd.read_csv('iris.csv')\n",
    "x = df.iloc[:, :2].to_numpy()\n",
    "y = df.replace(\"virginica\", \"versicolor\").values[:, -1]\n",
    "\n",
    "model = LDA()\n",
    "model.train(x, y)\n",
    "model.predict([x[0],x[-1],x[10]])\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['setosa', 'versicolor', 'setosa']"
      ]
     },
     "metadata": {},
     "execution_count": 485
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Now implement the kernel-based logic in a KernelLDA classifier. The kernel should be passed as an argument kernel at initialization time, which accepts two elements of the input space and produces their similarity value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "source": [
    "def polynomialKernel(x1, x2, c=0, q=1):\n",
    "    return pow(c+np.matmul(x1, x2), q)\n",
    "\n",
    "\n",
    "def linearKernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "\n",
    "class KernelLDA():\n",
    "    def __init__(self, _kernel):\n",
    "        self._kernel = _kernel\n",
    "\n",
    "    def mapDataset(self, X, Y, corrected=True):\n",
    "        K = np.zeros((len(X), len(X)))\n",
    "        classes = list(np.unique(Y))\n",
    "        for i in range(len(K)):\n",
    "            for j in range(i):\n",
    "                K[i][j] = K[j][i] = self._kernel(X[i], X[j])\n",
    "            K[i][i] = self._kernel(X[i], X[i])\n",
    "\n",
    "\n",
    "        K_c = dict((el, np.zeros((len(X),list(Y).count(el)))) for el in classes)\n",
    "        for k in range(len(X)):\n",
    "            for j in range(K_c[y[k]].shape[1]):\n",
    "                K_c[y[k]][k][j]=self._kernel(X[k],X[j])\n",
    "        \n",
    "        if corrected:\n",
    "            K_1 = np.linalg.pinv(K) \n",
    "            for cla in classes:\n",
    "                K_c_1 = np.linalg.pinv(K_c[cla])\n",
    "                K_c[cla] = np.matmul(np.matmul(K_c[cla], K_c_1), K_c[cla])\n",
    "            return np.matmul(np.matmul(K, K_1), K), K_c\n",
    "        return K, K_c\n",
    "\n",
    "    def train(self, _X, _Y):\n",
    "        K, K_c = self.mapDataset(_X, _Y)\n",
    "        mean = {}\n",
    "        N = {}\n",
    "        for key in K_c:\n",
    "            n = K_c[key].shape[1]\n",
    "            mean[key] = np.matmul((K_c[key]/n), [1]*n)\n",
    "            N[key] = np.matmul( K_c[key] ,np.matmul((np.identity(n) - np.ones(n)/n), K_c[key].T))\n",
    "        N = np.sum(list(N.values()), axis=0)\n",
    "        means = list(mean.values())\n",
    "        diffMean = means[0]-means[1]\n",
    "        M = np.outer(diffMean, diffMean)\n",
    "        N_1 = np.linalg.inv(N)\n",
    "        N_1M = np.dot(N_1, M)\n",
    "\n",
    "        if np.linalg.det(N):\n",
    "            a = np.dot(N_1, diffMean)\n",
    "        else:\n",
    "            values, vectors = np.linalg.eig(N_1M)\n",
    "            a = vectors[:, values.argmax()]\n",
    "        a = a/np.sqrt(np.dot(a.T, np.dot(K, a)))\n",
    "\n",
    "df = pd.read_csv('iris.csv')\n",
    "x = df.iloc[:, :2].to_numpy()\n",
    "y = df.replace(\"virginica\", \"versicolor\").values[:, -1]\n",
    "\n",
    "model = KernelLDA(linearKernel)\n",
    "model.train(x, y)\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26557/1067082524.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKernelLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinearKernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_26557/1067082524.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, _X, _Y)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdiffMean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiffMean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiffMean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mN_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mN_1M\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Empirically check that the two algorithm have the same behavior if you use the linear kernel."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Implement generators for the polynomial kernel and the Gaussian kernel (so that you can choose the parameters c, q and σ when producing the kernel function)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Write a function to show a projection line w for some given dataset. The intercept should be chosen so that the line passes the mean of the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Implement the feature map belonging to the quadratic homogeneous kernel. Consider the PCA iris dataset with two classes. Explicitly transform the dataset with the feature map of the quadratic kernel, apply the LDA in the new dataset, and visualize the solution in a 3D plot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Create a function that takes a 2D database X with the ground truth labels y and a prediction vector yˆ. Create a scatter plot in which the different classes get different symbols, and they are scattered in green if the prediction is correct and in red if the prediction is wrong. Get predictions for the standard LDA and the Kernel LDA with different kernels (try also different parameters for each kernel) and plot the predictions for the Iris PCA dataset. Which algorithm produces best results?"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}