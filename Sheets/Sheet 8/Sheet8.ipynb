{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Integrantes\n",
    "* ### David Herrera\n",
    "* ### Estid Lozano"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Imports\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openml\n",
    "import math\n",
    "import pandas as pd\n",
    "import scipy.stats"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1\n",
    "### 1.1 Write a probabilistic learner LDA that builds models for binary classification via the linear discriminant analysis. Prediction should be made assuming a (1-dimensional) normal distribution for each class with means and variances according to the built model. When returning probabilities, normalize the densities assigned to each class so that the vector sums up to 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class LDA():\n",
    "    def train(self, _X, _Y):\n",
    "        data = {}\n",
    "        for x, y in zip(_X, _Y):\n",
    "            if y not in data.keys():\n",
    "                data[y] = []\n",
    "            data[y].append(x)\n",
    "        self.data = data.copy()\n",
    "        mean = {}\n",
    "        centerData = data\n",
    "        S = {}\n",
    "\n",
    "        for key in data:\n",
    "            mean[key] = np.mean(data[key], axis=0)\n",
    "            centerData[key] -= mean[key].T\n",
    "            S[key] = np.dot(centerData[key].T, centerData[key])\n",
    "\n",
    "        S = np.sum(list(S.values()), axis=0)\n",
    "        S_1 = np.linalg.inv(S)\n",
    "\n",
    "        self.means = mean\n",
    "        means = list(mean.values())\n",
    "        diffMean = means[0]-means[1]\n",
    "\n",
    "        B = np.outer(diffMean, diffMean)\n",
    "        S_1B = np.dot(S_1, B)\n",
    "        w = None\n",
    "        if np.linalg.det(S):\n",
    "            w = np.dot(S_1, diffMean)\n",
    "            w = w/np.linalg.norm(w)\n",
    "        else:\n",
    "            values, vectors = np.linalg.eig(S_1B)\n",
    "            w = vectors[:, values.argmax()]\n",
    "        self.w = w\n",
    "        return w\n",
    "\n",
    "    def predict(self, _X):\n",
    "        res = []\n",
    "        for x in _X:\n",
    "            resTemp = []\n",
    "            x = np.dot(self.w, x)\n",
    "            for key in self.data:\n",
    "                projectedPoints = np.array([np.dot(self.w, x) for x in self.data[key]])\n",
    "                # projectedMean=np.mean(projectedPoints)\n",
    "                projectedMean = np.dot(self.w, self.means[key])\n",
    "                variance = np.var(projectedPoints)\n",
    "                normal = scipy.stats.norm(projectedMean, variance).pdf(x)\n",
    "                resTemp.append(normal)\n",
    "            resTemp = resTemp/np.linalg.norm(resTemp)\n",
    "            res.append(list(self.data.keys())[resTemp.argmax()])\n",
    "        return res\n",
    "\n",
    "\n",
    "df = pd.read_csv('iris.csv')\n",
    "x = df.iloc[:, :2].to_numpy()\n",
    "y = df.replace(\"virginica\", \"versicolor\").values[:, -1]\n",
    "\n",
    "model = LDA()\n",
    "model.train(x, y)\n",
    "model.predict([x[0],x[-1],x[10]])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Now implement the kernel-based logic in a KernelLDA classifier. The kernel should be passed as an argument kernel at initialization time, which accepts two elements of the input space and produces their similarity value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def linearKernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "\n",
    "class KernelLDA():\n",
    "    def __init__(self, _kernel):\n",
    "        self._kernel = _kernel\n",
    "\n",
    "    def mapDataset(self, _X, _Y, corrected=True):\n",
    "        K = np.zeros((len(_X), len(_X)))\n",
    "        classes = list(np.unique(_Y))\n",
    "        for i in range(len(K)):\n",
    "            for j in range(i):\n",
    "                K[i][j] = K[j][i] = self._kernel(_X[i], _X[j])\n",
    "            K[i][i] = self._kernel(_X[i], _X[i])\n",
    "\n",
    "\n",
    "        K_c = dict((el, np.zeros((len(_X),list(_Y).count(el)))) for el in classes)\n",
    "        for k in range(len(_X)):\n",
    "            for j in range(K_c[_Y[k]].shape[1]):\n",
    "                K_c[_Y[k]][k][j]=self._kernel(_X[k],_X[j])\n",
    "        \n",
    "        if corrected:\n",
    "            K_1 = np.linalg.pinv(K) \n",
    "            for cla in classes:\n",
    "                K_c_1 = np.linalg.pinv(K_c[cla])\n",
    "                K_c[cla] = np.matmul(np.matmul(K_c[cla], K_c_1), K_c[cla])\n",
    "            return np.matmul(np.matmul(K, K_1), K), K_c\n",
    "        return K, K_c\n",
    "\n",
    "    def train(self, _X, _Y):\n",
    "        K, K_c = self.mapDataset(_X, _Y)\n",
    "        mean = {}\n",
    "        N = {}\n",
    "        for key in K_c:\n",
    "            n = K_c[key].shape[1]\n",
    "            mean[key] = np.matmul((K_c[key]/n), [1]*n)\n",
    "            N[key] = np.matmul( K_c[key] ,np.matmul((np.identity(n) - np.ones(n)/n), K_c[key].T))\n",
    "        N = np.sum(list(N.values()), axis=0)\n",
    "        N = K\n",
    "        means = list(mean.values())\n",
    "        diffMean = means[0]-means[1]\n",
    "        M = np.outer(diffMean, diffMean)\n",
    "        N_1 = np.linalg.inv(N)\n",
    "        N_1M = np.dot(N_1, M)\n",
    "\n",
    "        if np.linalg.det(N):\n",
    "            a = np.dot(N_1, diffMean)\n",
    "        else:\n",
    "            values, vectors = np.linalg.eig(N_1M)\n",
    "            a = vectors[:, values.argmax()]\n",
    "        a = a/np.sqrt(np.dot(a.T, np.dot(K, a)))\n",
    "\n",
    "df = pd.read_csv('iris.csv')\n",
    "xa = df.iloc[:, :2].to_numpy()\n",
    "ya = df.replace(\"virginica\", \"versicolor\").values[:, -1]\n",
    "\n",
    "model = KernelLDA(linearKernel)\n",
    "model.train(xa, ya)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Empirically check that the two algorithm have the same behavior if you use the linear kernel."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Implement generators for the polynomial kernel and the Gaussian kernel (so that you can choose the parameters c, q and σ when producing the kernel function)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def gaussianKernel(x1, x2, sigma=1):\n",
    "    return math.exp(-pow(np.linalg.norm(x1-x2),2)/(2*pow(sigma,2)))\n",
    "# polynomial\n",
    "def polynomialKernel(x1, x2, c=0, q=1):\n",
    "    return pow(c+np.matmul(x1,x2),q)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Write a function to show a projection line w for some given dataset. The intercept should be chosen so that the line passes the mean of the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def showProjectionLine(_w,_x,_y):\n",
    "    fig, ax = plt.subplots()\n",
    "    classes = list(np.unique(_y))\n",
    "    mean = np.mean(_x,axis=0)\n",
    "    X1 = _x[_y == classes[0]]\n",
    "    X2 = _x[_y == classes[1]]\n",
    "    ax.scatter(X1[:,0], X1[:,1])\n",
    "    ax.scatter(X2[:,0], X2[:,1])\n",
    "    slope = _w[0] / _w[1]\n",
    "    intercept = -slope*mean[0]+mean[1]\n",
    "    a2_min, a2_max = min(_x[:,1]), max(_x[:,1])\n",
    "    q1 = (a2_max - intercept) / slope\n",
    "    q2 = (a2_min - intercept) / slope\n",
    "    x_from = max(min(_x[:,0]), min(q1, q2))\n",
    "    x_to = min(max(_x[:,0]), max(q1, q2))\n",
    "    hp = lambda x: x * slope + intercept\n",
    "    domain = np.linspace(x_from, x_to, 100)\n",
    "    ax.plot(domain, hp(domain), color=\"black\")\n",
    "    \n",
    "df = pd.read_csv('iris.csv')\n",
    "x = df.iloc[:, :2].to_numpy()\n",
    "y = df.replace(\"virginica\", \"versicolor\").values[:, -1]\n",
    "model = LDA()\n",
    "w = model.train(x, y)\n",
    "showProjectionLine(w,x,y)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Implement the feature map belonging to the quadratic homogeneous kernel. Consider the PCA iris dataset with two classes. Explicitly transform the dataset with the feature map of the quadratic kernel, apply the LDA in the new dataset, and visualize the solution in a 3D plot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# %matplotlib inline\n",
    "%matplotlib widget\n",
    "df = pd.read_csv('iris_pca_notseparable.csv')\n",
    "quadraticMap = lambda x1: [np.power(x1[0],2),np.power(x1[1],2),np.sqrt(2)*x1[0]*x1[1]]\n",
    "X = df.iloc[:, :2].to_numpy()\n",
    "Y = df.iloc[:, -1].to_numpy()\n",
    "X = np.array([quadraticMap(x) for x in X])\n",
    "model = LDA()\n",
    "w = model.train(X,Y)\n",
    "\n",
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "ax = ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "classes= np.unique(Y)\n",
    "X1 = X[Y == classes[0]]\n",
    "X2 = X[Y == classes[1]]\n",
    "ax.scatter(X1[:,0], X1[:,1], X1[:,2])\n",
    "ax.scatter(X2[:,0], X2[:,1], X2[:,2])\n",
    "\n",
    "\n",
    "ax = ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "projectedX1 =  np.array([np.dot(w,np.dot(xa,w.T)) for xa in X1])\n",
    "projectedX2 =  np.array([np.dot(w,np.dot(xa,w.T)) for xa in X2])\n",
    "ax.scatter(projectedX1[:,0], projectedX1[:,1], projectedX1[:,2])\n",
    "ax.scatter(projectedX2[:,0], projectedX2[:,1], projectedX2[:,2])\n",
    "\n",
    "t = np.linspace(-5,15,50)\n",
    "x = w[0]*t\n",
    "y = w[1]*t\n",
    "z = w[2]*t\n",
    "ax.plot(x,y,z)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Create a function that takes a 2D database X with the ground truth labels y and a prediction vector yˆ. Create a scatter plot in which the different classes get different symbols, and they are scattered in green if the prediction is correct and in red if the prediction is wrong. Get predictions for the standard LDA and the Kernel LDA with different kernels (try also different parameters for each kernel) and plot the predictions for the Iris PCA dataset. Which algorithm produces best results?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib widget\n",
    "def plotDiff(_X,_Yt,_Yp):\n",
    "    fig, ax = plt.subplots()\n",
    "    classes = np.unique(_Yt)\n",
    "    X1 = _X[_Yt == classes[0]]\n",
    "    X2 = _X[_Yt == classes[1]]\n",
    "    diff= _Yt == _Yp\n",
    "    ax.scatter(X1[:,0], X1[:,1],marker='*',color=[\"green\" if temp else \"blue\" for temp in diff[_Yt == classes[0]] ] )\n",
    "    ax.scatter(X2[:,0], X2[:,1],marker='.',color=[\"green\" if temp else \"blue\" for temp in diff[_Yt == classes[1]] ] )\n",
    "\n",
    "df = pd.read_csv('iris_pca_notseparable.csv')\n",
    "X = df.iloc[:, :2].to_numpy()\n",
    "Y = df.iloc[:, -1].to_numpy()\n",
    "\n",
    "model = LDA()\n",
    "model.train(X, Y)\n",
    "ypredicted = model.predict(X)\n",
    "plotDiff(X,Y,ypredicted)\n",
    "\n",
    "# model = KernelLDA(gaussianKernel)\n",
    "# model.train(X, Y)\n",
    "# ypredicted = model.predict(X)\n",
    "# plotDiff(X,Y,ypredicted)\n",
    "\n",
    "\n",
    "# model = KernelLDA(polynomialKernel)\n",
    "# model.train(X, Y)\n",
    "# ypredicted = model.predict(X)\n",
    "# plotDiff(X,Y,ypredicted)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}