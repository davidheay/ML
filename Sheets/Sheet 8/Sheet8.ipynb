{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Integrantes\n",
    "* ### David Herrera\n",
    "* ### Estid Lozano"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "source": [
    "# Imports\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openml\n",
    "import pandas as pd\n",
    "import scipy.stats"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1\n",
    "### 1.1 Write a probabilistic learner LDA that builds models for binary classification via the linear discriminant analysis. Prediction should be made assuming a (1-dimensional) normal distribution for each class with means and variances according to the built model. When returning probabilities, normalize the densities assigned to each class so that the vector sums up to 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "source": [
    "class LDA():\n",
    "    def train(self, _X, _Y):\n",
    "        data = {}\n",
    "        for x, y in zip(_X, _Y):\n",
    "            if y not in data.keys():\n",
    "                data[y] = []\n",
    "            data[y].append(x)\n",
    "        self.data = data.copy()\n",
    "        mean = {}\n",
    "        centerData = data\n",
    "        S = {}\n",
    "\n",
    "        for key in data:\n",
    "            mean[key] = np.mean(data[key], axis=0)\n",
    "            centerData[key] -= mean[key].T\n",
    "            S[key] = np.dot(centerData[key].T, centerData[key])\n",
    "\n",
    "        S = np.sum(list(S.values()), axis=0)\n",
    "        S_1 = np.linalg.inv(S)\n",
    "\n",
    "        self.means = mean\n",
    "        means = list(mean.values())\n",
    "        diffMean = means[0]-means[1]\n",
    "\n",
    "        B = np.outer(diffMean, diffMean)\n",
    "        S_1B = np.dot(S_1, B)\n",
    "        w = None\n",
    "        if np.linalg.det(S):\n",
    "            w = np.dot(S_1, diffMean)\n",
    "            w = w/np.linalg.norm(w)\n",
    "        else:\n",
    "            values, vectors = np.linalg.eig(S_1B)\n",
    "            w = vectors[:, values.argmax()]\n",
    "        self.w = w\n",
    "\n",
    "    def predict(self, _X):\n",
    "        res = []\n",
    "        for x in _X:\n",
    "            resTemp = []\n",
    "            x = np.dot(self.w, x)\n",
    "            for key in self.data:\n",
    "                projectedPoints = np.array([np.dot(self.w, x) for x in self.data[key]])\n",
    "                # projectedMean=np.mean(projectedPoints)\n",
    "                projectedMean = np.dot(self.w, self.means[key])\n",
    "                variance = np.var(projectedPoints)\n",
    "                normal = scipy.stats.norm(projectedMean, variance).pdf(x)\n",
    "                resTemp.append(normal)\n",
    "            resTemp = resTemp/np.linalg.norm(resTemp)\n",
    "            res.append(list(self.data.keys())[resTemp.argmax()])\n",
    "        return res\n",
    "\n",
    "\n",
    "df = pd.read_csv('iris.csv')\n",
    "x = df.iloc[:, :2].to_numpy()\n",
    "y = df.replace(\"virginica\", \"versicolor\").values[:, -1]\n",
    "\n",
    "model = LDA()\n",
    "model.train(x, y)\n",
    "model.predict([x[0],x[-1],x[10]])\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['setosa', 'versicolor', 'setosa']"
      ]
     },
     "metadata": {},
     "execution_count": 214
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Now implement the kernel-based logic in a KernelLDA classifier. The kernel should be passed as an argument kernel at initialization time, which accepts two elements of the input space and produces their similarity value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Empirically check that the two algorithm have the same behavior if you use the linear kernel."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Implement generators for the polynomial kernel and the Gaussian kernel (so that you can choose the parameters c, q and σ when producing the kernel function)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Write a function to show a projection line w for some given dataset. The intercept should be chosen so that the line passes the mean of the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Implement the feature map belonging to the quadratic homogeneous kernel. Consider the PCA iris dataset with two classes. Explicitly transform the dataset with the feature map of the quadratic kernel, apply the LDA in the new dataset, and visualize the solution in a 3D plot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Create a function that takes a 2D database X with the ground truth labels y and a prediction vector yˆ. Create a scatter plot in which the different classes get different symbols, and they are scattered in green if the prediction is correct and in red if the prediction is wrong. Get predictions for the standard LDA and the Kernel LDA with different kernels (try also different parameters for each kernel) and plot the predictions for the Iris PCA dataset. Which algorithm produces best results?"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}