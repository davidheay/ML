{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1698abb1-17ea-4eef-bc8f-9197cdb12a29",
   "metadata": {},
   "source": [
    "### David Herrera\n",
    "### Estid Lozano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d1700-3dfc-4c00-9074-30ba62c8a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import openml as oml\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce645fa2-f08a-4db0-8d23-b342d3a7c784",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381dad55-b841-41c6-97d0-050f766b4fac",
   "metadata": {},
   "source": [
    "**2 pts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bccd54e-20db-48af-92f0-f3a717cb3f1f",
   "metadata": {},
   "source": [
    "Let $K$ be the Gaussian kernel and let $D = \\{(x_1, y_1), ..., (x_n, y_n)\\}$ be a dataset. Now consider the empirical reproducing kernel map $φ$ with $φ(x) = K(x, ·)$, and suppose that we use this kernel map to generate new features based on these “landmarks”. Formally, let $D' = \\{(φ(x)(D), y) | (x, y) ∈ D\\}$ be the dataset we obtain if we have one feature for each datapoint indicating the similarity as per the Gaussian kernel $K$. So $D' ∈ R^{n×n}$.\n",
    "\n",
    "Show that it is wrong to say that kernels produce new features by showing that the dot product of two arbitrary points $x_i$, $x_j$ in $D'$ is not identical to the kernel value $K(x_i, x_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48273076-474e-477d-87e2-c1459ace4966",
   "metadata": {},
   "source": [
    "**proof**:\n",
    "\n",
    "$$\n",
    "\\\\ K(x_i, x_j) = exp\\{-\\frac{\\Vert{x_i-x_j}\\Vert^2}{2\\sigma^2}\\} \n",
    "\\\\ \\phi(x) = (K(x_1,x),K(x_2,x),...,K(x_n,x))^T\n",
    "\\\\ \\phi(x_i)^T \\phi(x_j) = \\sum_{k=1}^n(K(x_k,x_i)K(x_k,x_j))\n",
    "\\\\ = \\sum_{k=1}^n(\n",
    "exp\\{-\\frac{\\Vert{x_k-x_i}\\Vert^2}{2\\sigma^2}\\}\n",
    "exp\\{-\\frac{\\Vert{x_k-x_j}\\Vert^2}{2\\sigma^2}\\})\n",
    "\\\\ = \\sum_{k=1}^n(exp\\{-\\frac{\\Vert{x_k-x_i}\\Vert^2+\\Vert{x_k-x_j}\\Vert^2}{2\\sigma^2}\\})\n",
    "\\neq exp\\{-\\frac{\\Vert{x_i-x_j}\\Vert^2}{2\\sigma^2}\\} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb280b-51f1-4f1d-94bb-58ee17c71802",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0873911-847a-4b14-a7d1-e93974038819",
   "metadata": {},
   "source": [
    "**(Raw Empirical vs. corrected Empirical Kernel Map - 4 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd622d3-af9a-4c9b-b58b-262e153528d2",
   "metadata": {},
   "source": [
    "**2.1.** Implement the polynomial kernel and the Gaussian kernel explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9db701-521b-4a92-becd-fc26c4fed9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial\n",
    "def polynomialKernel(x1, x2, c=0, q=1):\n",
    "    return pow(c+np.matmul(x1,x2),q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feff0dd-f4c3-4fc8-9bf8-8f427a7a1d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian\n",
    "def gaussianKernel(x1, x2, sigma=1):\n",
    "    return math.exp(-pow(np.linalg.norm(x1-x2),2)/(2*pow(sigma,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841a8db-1f29-4a9c-8891-cab01d2762f0",
   "metadata": {},
   "source": [
    "**2.2.** Write a function **mapDataset(X, kernel, corrected=True)** that takes a dataset (only the attribute not the labels) and explicitly computes the empirical feature map of the given kernel (functional argument). Depending on whether **corrected** is true, it should adjust the data using the $K^{−1/2}$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1cce4-7c34-47f7-a240-69218d58d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapDataset(X, kernel, corrected=True):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.to_numpy()\n",
    "    # compute empirical feature map\n",
    "    K = np.empty((len(X), len(X)))\n",
    "    for i in range(len(K)):\n",
    "        for j in range(i):\n",
    "            K[i][j] = K[j][i] = kernel(X[i], X[j])\n",
    "        K[i][i] = kernel(X[i], X[i])\n",
    "    # corrected\n",
    "    if corrected:\n",
    "        K_1 = np.linalg.pinv(K) # lol\n",
    "        return np.matmul(np.matmul(K, K_1), K)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b982a972-86a0-42d4-a2dc-f0eeaa8c0c68",
   "metadata": {},
   "source": [
    "**2.3.** Explicitly test that using the corrected version, the kernel computes the correct dot product in the feature space when applying both polynomial and Gaussian kernel to the iris, amazon, and madelon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d47ce7d-f15b-4449-9c8c-607268ffa290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(_id):\n",
    "    dtset = oml.datasets.get_dataset(_id)\n",
    "    X, y, catInd, attrs = dtset.get_data()\n",
    "    if y is None and attrs[-1].lower() == \"class\":\n",
    "        X, y = X.iloc[:,:-1], X.iloc[:,-1]\n",
    "    return dtset.name, X.to_numpy(), y.to_numpy()\n",
    "\n",
    "datasets = [getDataset(i) for i in [61, 1457, 1485]]\n",
    "kernels = [polynomialKernel, gaussianKernel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd388ed-157d-41d2-a373-c56cd7b3840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "for dtset in datasets:\n",
    "    name, X, y = dtset\n",
    "    print(name)\n",
    "    startTime = time.monotonic()\n",
    "    for kernel in kernels:\n",
    "        print(\"- \"+kernel.__name__)\n",
    "        mapped = mapDataset(X, kernel)\n",
    "        print(mapped)\n",
    "    print(round(time.monotonic() - startTime, 3), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59bb34b-8e86-4744-a650-03e7c15ae073",
   "metadata": {},
   "source": [
    "**2.4.** Report performance of Logistic Regression, Naive Bayes and Decision Trees when applied to these three (mapped) datasets using the linear kernel, the quadratic kernel (each of which with $c ∈ \\{0, 1, 10\\}$) and with the Gaussian kernel (for values of $σ ∈ \\{0.5, 1, 10\\}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a1c0da-0b85-436a-9cf4-4e6992b4a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [LogisticRegression, GaussianNB, DecisionTreeClassifier]\n",
    "linearKernel = lambda x1, x2, c=0: polynomialKernel(x1, x2, c, 1) # only for the name\n",
    "quadraticKernel = lambda x1, x2, c=0: polynomialKernel(x1, x2, c, 2)\n",
    "linearKernel.__name__, quadraticKernel.__name__ = \"linearKernel\", \"quadraticKernel\"\n",
    "kernels = [linearKernel, quadraticKernel, gaussianKernel]\n",
    "params = [(\"c\", [0, 1, 10])] * 2 + [(\"sigma\", [0.5, 1, 10])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde82430-47b0-4832-9e3e-d602f33185c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance\n",
    "for dtset in datasets:\n",
    "    dtsetName, X, y = dtset\n",
    "    print(dtsetName)\n",
    "    dtsetTime = time.monotonic()\n",
    "    for ik, kernel in enumerate(kernels):\n",
    "        print(\"* \"+kernel.__name__)\n",
    "        paramName, paramList = params[ik]\n",
    "        kernelTime = time.monotonic()\n",
    "        for param in paramList:\n",
    "            print(\"  + \"+paramName+\": \"+str(param))\n",
    "            XNew = mapDataset(X, kernel)\n",
    "            XTrain, XTest, yTrain, yTest = train_test_split(XNew, y, test_size=0.8, random_state=0)\n",
    "            for clf in classifiers:\n",
    "                yPred = clf().fit(XTrain, yTrain).predict(XTest)\n",
    "                print(\"    - \"+clf.__name__+\" performance: \"+str(round(1-(yTest != yPred).sum()/len(y), 3)))\n",
    "        print(\"  + \"+kernel.__name__+\" time\", round(time.monotonic() - kernelTime, 3), \"seconds\")\n",
    "    print(\"* \"+dtsetName+\" time\", round(time.monotonic() - dtsetTime, 3), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53f19b7-51a7-43a0-bb37-25127080cb7e",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9cf13b-949b-474b-abc8-4909c43c8ec4",
   "metadata": {},
   "source": [
    "**2 pts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87dd2c1-b3e1-4308-a3d9-1a85663a32f9",
   "metadata": {},
   "source": [
    "Create an artificial dataset with two attributes and two classes. There should be 100 instances in the range of 1 around the origin and 400 instances outside the unit sphere (but in a range of less than 10) uniformly distributed. Visualize your data. Now map your data with the feature map corresponding to the quadratic kernel for different values of c into a 3D-space. Create 3D plots (with appropriate axes labels) and explain what you observe and in how far linear separability is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df09d609-b926-4f4c-ae1a-15655323e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artificial dataset\n",
    "Angles = np.random.uniform(0, 2 * math.pi, 500)\n",
    "Distances = np.append(np.random.uniform(0, 1, 100), np.random.uniform(1, 10, 400))\n",
    "X = np.column_stack((Distances * np.cos(Angles), Distances * np.sin(Angles)))\n",
    "y = np.append(np.repeat(0, 100), np.repeat(1, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3931f926-6cf4-4a00-9ad6-0a30f6d015f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "plt.figure(figsize = (10, 10))\n",
    "colors = [\"r\", \"g\"]\n",
    "for i, c in enumerate(np.unique(y)):\n",
    "    filt = np.where(y == c)[0]\n",
    "    plt.scatter(X[filt, 0], X[filt, 1], s = 10, c = colors[i], alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082f7cd-2100-4e80-a79c-184d8b599a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69840282-0ea1-44a6-8f85-4b3012178a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d07b83a-aaaf-4d02-9d68-f4e156e44e4c",
   "metadata": {},
   "source": [
    "**Answer:** We observe..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
