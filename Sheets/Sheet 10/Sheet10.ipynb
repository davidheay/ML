{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51bd0b6e-0388-413b-9e3e-b82ecece8606",
   "metadata": {},
   "source": [
    "## Estid Lozano\n",
    "## David Herrera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f12d29-1508-4e54-ac48-05e6fecbe57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, TimeDistributed, LSTM\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from random import randint, uniform\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74f1e5-3f2f-4443-99bb-c263a5feb6b9",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf308acb-5f89-4e53-940c-622dc4247e50",
   "metadata": {},
   "source": [
    "Familiarize yourself with the keras machine learning library and\n",
    "check the following tutorial to learn how to use the time distributed layer to implement a many-to-many LSTM network: https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/.\n",
    "\n",
    "We now want to to use this architecture in order to predict all items of a sequence that are bigger than some given value.\n",
    "\n",
    "For example, we have a sequence x = (1, 4, 2, 5, 1, 1, 6) and a threshold of 3. Then the output sequence should be y = (0, 1, 0, 1, 0, 0, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc16aa2-2d5f-4b19-88ae-9d2ea579dbd7",
   "metadata": {},
   "source": [
    "**1.1.** Create a recurrent neural network with keras with one LSTM layer with a single unit. Make sure that the network is not fixed so sequences of a particular length but can receive sequences of arbitrary length.\n",
    "\n",
    "Since we conduct binary classification (over sequences), use the **binary_crossentropy** as the loss function.\n",
    "\n",
    "Hint: Check the possibility to leave the time dimension in the input shape as **None**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9fce6-42ce-4e94-b58d-57f5425579ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_neurons, input_shape=(None, 1), return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788719db-3291-4c85-9d80-a34941bfef46",
   "metadata": {},
   "source": [
    "**1.2.** Write a *generator* function (check the **yield** command) that produces 100 training examples of random length between 20 and 50 (all sequences of the same batch may have the same length). Each training example $x_i$ is a sequence of numbers with (float) values between 0 and 10, and the corresponding label sequence $y_i$ should check whether the numbers are at least 10. That is, $y_i$ is a boolean vector of the same length as $x_i$.\n",
    "\n",
    "Use the **model.fit_generator** command to train the model (choose an appropriate number of epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d207a-785d-4277-b835-feb6f59c2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(n=100, min_len=20, max_len=50, min_val=0, max_val=10, threshold=3):\n",
    "    X = [np.array([uniform(min_val, max_val) for i in range(randint(min_len, max_len))]) for i in range(n)]\n",
    "    y = [(x >= threshold).astype(int) for x in X]\n",
    "    X = [i.reshape(1, len(i), 1) for i in X]\n",
    "    y = [i.reshape(1, len(i), 1) for i in y]\n",
    "    while True:\n",
    "        for i in range(n):\n",
    "            yield X[i], y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df242bb-0ae2-4d7a-89fd-b58c22d839cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 20\n",
    "n_train = 100\n",
    "\n",
    "train_gen = gen(n_train)\n",
    "model.fit(train_gen, epochs=n_epoch, steps_per_epoch=n_train, verbose=0, callbacks=[TqdmCallback(verbose=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e07ada-2466-4109-a179-54c519fd4a9a",
   "metadata": {},
   "source": [
    "Test your model on a number of randomly chosen sequences. Is it able to make correct\n",
    "predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d0c52-1413-473d-bf99-7e0f587534ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 10\n",
    "test_gen = gen(n_test)\n",
    "total_mistakes = 0\n",
    "total_predicts = 0\n",
    "\n",
    "for i in range(n_test):\n",
    "    xi, yi = next(test_gen)\n",
    "    yi = yi[0].T[0]\n",
    "    yi_hat = np.round(model.predict(xi)[0].T[0], 0).astype(int)\n",
    "    mistakes = sum(yi != yi_hat)\n",
    "    # print(\"Expected:\",yi,\"Predicted\",yi_hat,\"Mistakes:\",mistakes,\"/\",len(yi))\n",
    "    total_mistakes += mistakes\n",
    "    total_predicts += len(yi)\n",
    "acc = 1 - round(total_mistakes / total_predicts, 3)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"The model is \" + (\"\" if acc >= 0.9 else \"not \") + \"able to make correct predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7096a001-c5ba-417f-81b7-cd4fc99d48f9",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c441583f-c92a-4518-8923-eafa519f5259",
   "metadata": {},
   "source": [
    "In this exercise, we reproduce the experiments on the reber grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ead7c-32af-4082-860c-79af7764a407",
   "metadata": {},
   "source": [
    "**2.1.** Write a function to prepare a binary sequence database for a given symbolic database. Use one attribute for each possible symbol, and encode the x-sequences as sequences of binary vectors (with exactly one one for each symbol) and y-sequences as binary vectors with a 1 at the position of every possible next symbol (notated with | in the csv files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515537b9-29a9-4a1b-9175-45709a4ccd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66282725-4187-44b7-8be4-baba4db75356",
   "metadata": {},
   "source": [
    "Apply this function to the **reber1** and **reber2** datasets to produce binary sequence datasets. In this case, you should have sequences that have for each time step a 7-vector (for both x-sequences and y-sequences). If in doubt, look at the example in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cfbaa9-cfa6-4b7c-9a04-9ac2662ffe6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88d38136-8789-4ded-9792-f86710c28290",
   "metadata": {},
   "source": [
    "**2.2.** create a generator that returns batches of size one, each time an example from the dataset created in this way.\n",
    "\n",
    "Hint: Maybe you want to use the modulo operator % and maintain an iterator variable, so that you keep iterating over the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0ec7e-cb1e-49de-8fa7-1e3701755e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbf3ae31-166b-4df2-9afd-54792b4979ef",
   "metadata": {},
   "source": [
    "**2.3.** What are your results if using a RNN layer or LSTM layer respectively with 20 units and training only on the first 400 examples (and validating on the ultimate 100 sequences)? Can you learn to perfectly predict the next possible symbols?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f5bc9-9dad-4c82-bea2-845c9bd37baf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
