{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Integrantes\r\n",
    "* ### David Herrera\r\n",
    "* ### Estid Lozano"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.1** Consider a binary classification problem. Prove that the entropy of the data is 0 if there\r\n",
    "are only samples of one class and that it is maximal if there is exactly the same number\r\n",
    "of instances for both classes in the data. What is the valued of the entropy in that case?\r\n",
    "Hint: Compute the derivative of the entropy and show that it is 0 for a class probability\r\n",
    "of 0.5."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.2.** Consider a k-class classification problem. Determine analytically the exact range of\r\n",
    "values the Gini-Index can take."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.3** Consider a binary classification problem. Now for values of P(c1|D) 2 [0; 1], create plots\r\n",
    "for the entropy, the Gini-Index, and the CART-index."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now suppose that a dataset exhibits the maximum entropy. Further suppose that we\r\n",
    "have a split point that will split the dataset into exactly two equally large parts, i.e.,\r\n",
    "|DY| = |DN| = |D|=2. Now create a 3D surface plot in which you show the information\r\n",
    "gain for different combinations of values of H(DY) and H(DN)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.4.** A very simple rule-based learner is the so called Majority classifier, which always, without\r\n",
    "looking at the instance values, predicts the class that was most frequent in the training\r\n",
    "data.\r\n",
    "Show that the decision tree algorithm can be configured so that it behaves exactly like\r\n",
    "the majority classifier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Imports\r\n",
    "from numbers import Number\r\n",
    "import operator\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.1.** Implement the decision tree algorithm for both numerical and categorical attributes. For\r\n",
    "categorical attribute, you can assume that jV j = 1. As in the pseudo-code, treat the\r\n",
    "function Gain as a (functional) parameter of your algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "class DecisionTree:\r\n",
    "\r\n",
    "  def __init__(self, _n = 5, _pi = 0.95):\r\n",
    "    self._n = _n\r\n",
    "    self._pi = _pi\r\n",
    "\r\n",
    "  def train(self, X, Y):\r\n",
    "    self.X = X\r\n",
    "    self.Y = Y\r\n",
    "    # Pass D sorted (by Y) to the algorithm to skip some checks and searches\r\n",
    "    self.tree = self.run(sorted([[x, Y[i]] for i, x in enumerate(X)], key = lambda x : x[1]))\r\n",
    "      \r\n",
    "  def run(self, D):\r\n",
    "    n = len(D) # partition size\r\n",
    "    # get classes and their sizes\r\n",
    "    C, maxC = [(D[0][1], 0)], 0\r\n",
    "    for i, d in enumerate(D): # best way if sorted by Y (d[0])\r\n",
    "      if d[1] == C[-1][0]: # count class sizes\r\n",
    "        C[-1][1] += 1\r\n",
    "      else:\r\n",
    "        if C[-1][1] > C[maxC][1]: # to get maxC\r\n",
    "          maxC = len(C) - 1\r\n",
    "        C.append((d[1], 1))\r\n",
    "    if C[-1][1] > C[maxC][1]: # to get maxC\r\n",
    "      maxC = len(C) - 1\r\n",
    "    if (n <= self._n or (C[maxC][1] / n) >= self._pi): # stopping condition\r\n",
    "      return C[maxC][0]\r\n",
    "    split_, score_ = None, 0 # init best split point\r\n",
    "    for j, Dj in enumerate(D):\r\n",
    "      _v, score = None, 0\r\n",
    "      if isinstance(Dj[0], Number): # numerical\r\n",
    "        _v, score = self.evaluateNumericAttribute(D, C, j)\r\n",
    "      else: # categorical\r\n",
    "        _v, score = self.evaluateCategoricalAttribute(D, j)\r\n",
    "      if score > score_:\r\n",
    "        split_, score_ = (j, _v), score\r\n",
    "    # partition D into D_Y and D_N using split_, and call recursively\r\n",
    "    Y, N = None, None\r\n",
    "    if isinstance([split_[1]], Number): # numerical\r\n",
    "      Y, N = [d[0] <= split_[1] for d in D], map(operator.not_, Y) # satisfy, no satisfy (indexes)\r\n",
    "    else: # categorical\r\n",
    "      Y, N = [d[0] == split_[1] for d in D], map(operator.not_, Y) # satisfy, no satisfy (indexes)\r\n",
    "    return (split_, self.run(D[Y]), self.run(D[N]))\r\n",
    "\r\n",
    "  def predict(self, X):\r\n",
    "    return None\r\n",
    "\r\n",
    "  def evaluateNumericAttribute(self, D, C, j):\r\n",
    "    D = np.sort(np.copy(D), key=lambda x: x[j])\r\n",
    "    _M = set()\r\n",
    "    N_v = []\r\n",
    "    for i, d in enumerate(D):\r\n",
    "      # count already made\r\n",
    "      if D[i+1][0] != d[0]:\r\n",
    "        _v = (D[j+1][0]+d[0])/2\r\n",
    "        _M = _M.union(_v) # midpoints\r\n",
    "        for i, c in enumerate(C):\r\n",
    "          N_v.append(c[1]) # no. points Xj <= _v and Yj=Ci\r\n",
    "    # if D[-1][1] ==\r\n",
    "    #...\r\n",
    "    return v_, score_\r\n",
    "  \r\n",
    "  def evaluateCategoricalAttribute(self, D, j):\r\n",
    "    return None"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-66cc0037a90a>, line 53)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-66cc0037a90a>\"\u001b[1;36m, line \u001b[1;32m53\u001b[0m\n\u001b[1;33m    return None\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.2.** Create a function that can visualize the segmentation produced by a decision tree. Plot\r\n",
    "this segmentation for the iris dataset for the three different split point criteria (information\r\n",
    "gain, gini-index, and CART)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.3.** Consider values of _n E {1, 2, 5, 10} and _pi E {1.0, 0.9, 0.8, 0.5}. Create three 4 x 4 plot grids, one for each split criterion (information gain, gini-index, and CART) and each grid representing the parameter combinations of _n and _pi.\r\n",
    "In each cell, show the learning curves of the decision tree classifier on the breast cancer data using the respective values for _n and _pi. Do you observa any effect? For which case do you obtain the best results."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit (system)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "interpreter": {
   "hash": "39615471a12bbabdc24952a6b2334f200eb09c18349abce2bd8427685dd9b8c6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}