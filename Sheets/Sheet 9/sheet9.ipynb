{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estid Lozano\n",
    "### David Herrera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMNIST = pd.read_csv(\"mnist.csv\")\n",
    "dfMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    xp = x - max(x)\n",
    "    return np.exp(xp) / np.sum([np.exp(xi) for xi in xp])\n",
    "\n",
    "'''\n",
    "    Computes the Jacobian (matrix of derivatives) of a softmax layer given the outputs produced by that layer.\n",
    "    Attention: Do NOT provide the net values of the softmax layer here but the probability vector it produced.\n",
    "'''\n",
    "def softmax_deriv(z):\n",
    "    n = len(z)\n",
    "    d = np.zeros((n, n))\n",
    "    for i in range (n):\n",
    "        for j in range(i):\n",
    "            d[i,j] = d[j,i] = -(z[i] * z[j])\n",
    "        d[i,i] = z[i] * (1-z[i])\n",
    "    return d\n",
    "\n",
    "def cross_entropy_error(y, o):\n",
    "    return np.abs(np.dot(y.T,np.log(np.maximum(10**-20, o))))\n",
    "\n",
    "def cross_entropy_error_derivative(y, o):\n",
    "    return -y/np.maximum(10**-20,o)\n",
    "\n",
    "def get_activation(name):\n",
    "    if name == \"linear\":\n",
    "        return lambda x: x\n",
    "    if name == \"relu\":\n",
    "        return lambda x: np.maximum(0, x)\n",
    "    if name == \"softmax\":\n",
    "        return softmax\n",
    "\n",
    "def get_derivative(name):\n",
    "    if name == \"linear\":\n",
    "        return lambda x: np.eye(len(x))\n",
    "    if name == \"relu\":\n",
    "        return lambda x: np.diag((x > 0).astype(int).T[0])\n",
    "    if name == \"softmax\":\n",
    "        return softmax_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let q $\\in \\mathbb{N}$ and f:$\\mathbb{R} \\rightarrow \\mathbb{R}^q$ and g:$\\mathbb{R}^q \\rightarrow \\mathbb{R}$ two functions.\n",
    "\n",
    "$\\LARGE{ \\frac{\\delta g \\circ f}{\\delta x} \\stackrel{def}{=} \\frac{g(f(x))}{\\delta x} = \\sum^q_{i=1}\\frac{\\delta g}{\\delta f_i} \\cdot \\frac{\\delta f_i}{\\delta x} }$\n",
    "\n",
    "where $f_i$ refers to the i-th component in the output of $f$.\n",
    "Use this to show that\n",
    "\n",
    "$\\LARGE{ \\frac{\\delta \\varepsilon_x}{\\delta z^l} = \\sum^{n_{l+1}}_{k=1} \\frac{\\delta \\varepsilon_x}{\\delta net^{l+1}_k} \\cdot \\frac{\\delta net^{l+1}_k}{\\delta z^l} }$\n",
    "\n",
    "where $net^l_i$ is the net value of the i-th neuron in layer l. Conclude that\n",
    "\n",
    "$\\LARGE{ \\frac{\\delta \\varepsilon_x}{\\delta net^l_j} = \\frac{\\delta f^l(net^l_j)}{\\delta net^l_j} \\cdot \\sum^{n_{l+1}}_{k=1} \\frac{\\delta \\varepsilon_x}{\\delta net^{l+1}_k} \\cdot \\frac{\\delta net^{l+1}_k}{\\delta f^l(net^l_j)} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "Knowing that $\\LARGE{ \\frac{g(f(x))}{\\delta x} = \\sum^q_{i=1}\\frac{\\delta g}{\\delta f_i} \\cdot \\frac{\\delta f_i}{\\delta x} }$\n",
    "\n",
    "We have $\\LARGE{ \\frac{\\delta \\varepsilon_x}{\\delta net^l_j} = \\frac{\\delta \\varepsilon_x}{\\delta z^l} \\cdot \\frac{\\delta z^l}{\\delta net^l_j} }$\n",
    "\n",
    "Then, we replace $\\LARGE{ \\frac{\\delta \\varepsilon_x}{\\delta z^l} = \\sum^{n_{l+1}}_{k=1} \\frac{\\delta \\varepsilon_x}{\\delta net^{l+1}_k} \\cdot \\frac{\\delta net^{l+1}_k}{\\delta z^l} }$\n",
    "\n",
    "So we get $\\LARGE{ \\frac{\\delta \\varepsilon_x}{\\delta net^l_j} = \\frac{\\delta z^l}{\\delta net^l_j} \\cdot \\sum^{n_{l+1}}_{k=1} \\frac{\\delta \\varepsilon_x}{\\delta net^{l+1}_k} \\cdot \\frac{\\delta net^{l+1}_k}{\\delta z^l} }$\n",
    "\n",
    "Finally, we know that $\\LARGE{ z^l= f^l(net^l_j) }$\n",
    "\n",
    "Getting $\\LARGE{ \\frac{\\delta \\varepsilon_x}{\\delta net^l_j} = \\frac{\\delta f^l(net^l_j)}{\\delta net^l_j} \\cdot \\sum^{n_{l+1}}_{k=1} \\frac{\\delta \\varepsilon_x}{\\delta net^{l+1}_k} \\cdot \\frac{\\delta net^{l+1}_k}{\\delta f^l(net^l_j)} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement the neural network incrementally by extending\n",
    "the *ANNClassifier* class, which already comes with methods for setup and prediction. The classifier is set up with an array of descriptions of the hidden layer (one entry for each hidden layer). Your task is now to establish the learning process through back-propagation. Note that the last layer is, by definition, a softmax layer for classification or a linear layer for regression problems; in both cases, $\\delta^{h+1}=o-y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.** Implement a method *compute_net_gradients(z, delta_of_output_layer)* that computes a list containing the net gradients $\\delta^l$ for all layers (the output layer net gradient should also be contained). This is line 16 in the pseudo code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.** Implement a method *update_weights_and_biases(z, delta)* that realizes lines 17-22 in the pseudo code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.** Implement a method *step(x, y)* that updates the whole model for the instance x with its label y, i.e. that realizes the whole procedure of lines 9-22 for this instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4.** Implement a method *train(X, y)* that takes a set of given training data with its labels, invokes *setup* with the appropriate arguments, and realizes the outer loop (lines 7 + 8)\n",
    "of the algorithm relying on the method *step* for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNClassifier:\n",
    "    def __init__(self, architecture, learning_rate=1.0,max_iter = 10):\n",
    "        self.architecture = architecture\n",
    "        self.h = h = len(self.architecture)\n",
    "        self.n = [architecture[l][\"units\"] for l in range(h)]\n",
    "        self.f = [get_activation(architecture[l][\"activation\"]) for l in range(h)]\n",
    "        self.fd = [get_derivative(architecture[l][\"activation\"]) for l in range(h)]\n",
    "        self.W = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    def reset(self, input_dimension, labels, seed = 0):\n",
    "        self.labels = labels\n",
    "        num_classes = len(labels)\n",
    "        layer_sizes = [input_dimension] + self.n + [num_classes]\n",
    "        self.b = []\n",
    "        self.W = []\n",
    "        np.random.seed(seed)\n",
    "        for l in range(self.h):\n",
    "            self.b.append(np.random.uniform(low=-0.01,high=0.01,size=(layer_sizes[l + 1], 1)))\n",
    "            self.W.append(np.random.uniform(low=-0.01,high=0.01,size=(layer_sizes[l], layer_sizes[l + 1])))\n",
    "        self.b.append(np.random.uniform(low=-0.01,high=0.01,size=(layer_sizes[-1], 1)))\n",
    "        self.W.append(np.random.uniform(low=-0.01,high=0.01,size=(layer_sizes[-2], layer_sizes[-1])))\n",
    "        self.f.append(get_activation(\"softmax\"))\n",
    "        self.fd.append(get_derivative(\"softmax\"))\n",
    "    \n",
    "    def forward_phase(self, x):\n",
    "        z = [x.reshape(len(x), 1)]\n",
    "        for l in range(self.h + 1):\n",
    "            net = np.dot(self.W[l].T, z[l]).reshape(len(self.b[l]), 1) + self.b[l]\n",
    "            if any(np.isnan(net)):\n",
    "                raise Exception(\"Found nan-value in net: \" + str(net))\n",
    "            zl = self.f[l](net)\n",
    "            if any(np.isnan(zl)):\n",
    "                raise Exception(\"Found nan-value in z-vector after applying activation function \" + str(self.f[l]) + \" to arguments \" + str(net) + \": \" + str(zl) + \". These is the complete list of computed z-valuies: \" + str(z))\n",
    "            z.append(zl)\n",
    "        return z\n",
    "    \n",
    "    def step(self, x, y):\n",
    "        z = self.forward_phase(x)\n",
    "        oi = z[-1]\n",
    "        deltas = self.compute_net_gradients(z, oi - y.reshape(len(y), 1))\n",
    "        self.update_weights_and_biases(z, deltas)\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        self.reset(X.shape[1], list(np.unique(y,axis=0)))\n",
    "        # Setup\n",
    "        for t in range(self.max_iter + 1):\n",
    "            random_indexes = list(range(len(X)))\n",
    "            random.shuffle(random_indexes)\n",
    "            for i in random_indexes:\n",
    "                self.step(X[i], y[i])\n",
    "    \n",
    "    def compute_net_gradients(self, z, delta_of_output_layer):\n",
    "        # Backpropagation Phase\n",
    "        deltas = [None] * (self.h + 2)\n",
    "        deltas[-1] = delta_of_output_layer\n",
    "        for l in range(self.h, 0, -1):\n",
    "            deltas[l] = np.dot(self.fd[l](z[l]), np.dot(self.W[l], deltas[l + 1]))\n",
    "        return deltas\n",
    "    \n",
    "    def update_weights_and_biases(self, z, deltas):\n",
    "        # Gradient Descent Step\n",
    "        grads_W = [None] * (self.h + 1)\n",
    "        grads_b = [None] * (self.h + 1)\n",
    "        for l in range(self.h + 1):\n",
    "            grads_W[l] = np.dot(z[l], deltas[l + 1].T)\n",
    "            grads_b[l] = deltas[l + 1]\n",
    "        for l in range(self.h + 1):\n",
    "            self.W[l] = self.W[l]- self.learning_rate * grads_W[l]\n",
    "            self.b[l] = self.b[l]- self.learning_rate * grads_b[l]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [self.labels[np.argmax(self.forward_phase(x)[-1])] for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.** Train 10 networks with 1 to 10 units in a single hidden layer on the iris dataset. What\n",
    "do you observe? Plot the in-sample prediction results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.** Download the MNIST dataset from *https://www.openml.org/data/get_csv/52667/mnist_784.arff*. Try 10 networks with one hidden layer and between 1 and 400 units in this layer. Try learning rates {$1,10^{-1},...,10^{-4}$} and report your learning curves (see script). In addition, also try some other configurations with more hidden layers. In this exercise, never user any of the last 10000 instances during the training process.\n",
    "\n",
    "Which architecture achieves best performance? Plot the predictions with the respective function for at least 100 instances in the test fold (last 10000 instances).\n",
    "\n",
    "Also observe whether scaling the data into the [0,1] interval (dividing by 255) changes\n",
    "the algorithm behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "architecture = [\n",
    "    {\n",
    "        \"units\": 100,\n",
    "        \"activation\": \"relu\"\n",
    "    }\n",
    "]\n",
    "ann = ANNClassifier(architecture, learning_rate = 10**-2, max_iter = 1)\n",
    "X = dfMNIST.values[:,:-1].astype(float)\n",
    "y = dfMNIST.values[:,-1].reshape(len(dfMNIST), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d = X.shape[1]\n",
    "ann.reset(d, list(np.unique(y)))\n",
    "ann.predict(X[60000:])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "def visualizeLearningProcess(ann, X_train, y_train, X_test, y_test, max_epochs = 100, plot_step_size = 100):\n",
    "    d = X_train.shape[1]\n",
    "    ann.reset(d, list(np.unique(y_train)))\n",
    "    \n",
    "    lc = []\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.show()\n",
    "    fig.canvas.draw()\n",
    "    plt.ion()\n",
    "    queue = []\n",
    "    epochs = 0\n",
    "    steps = max_epochs * X_train.shape[0]\n",
    "    for t in tqdm(range(steps)):\n",
    "        \n",
    "        # get example\n",
    "        if len(queue) == 0:\n",
    "            epochs += 1\n",
    "            queue = random.sample(range(len(X_train)), len(X_train))\n",
    "            \n",
    "        # update model\n",
    "        index = queue[0]\n",
    "        del queue[0]\n",
    "        x = X_train[index].reshape(d, 1)\n",
    "        y = y_train[index]\n",
    "        ann.step(x, y)\n",
    "        \n",
    "        if t % plot_step_size == 0:\n",
    "            # compute error rate\n",
    "            y_hat = np.array(ann.predict(X_test)).reshape(len(y_test), 1)\n",
    "            error_rate = np.count_nonzero(y_hat != y_test) / len(y_hat)\n",
    "            lc.append(error_rate)\n",
    "\n",
    "            # update learning curve\n",
    "            ax.clear()\n",
    "            ax.plot(plot_step_size * np.array(range(len(lc))), lc)\n",
    "            ax.set_ylim([0, .9])\n",
    "            ax.axhline(min(lc), linestyle=\"--\", color=\"red\")\n",
    "            ax.set_xlabel(\"Training points\")\n",
    "            fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualizeLearningProcess(ann, X[:60000], y[:60000], X[60000:], y[60000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualizeLearningOnIris():\n",
    "    %matplotlib inline\n",
    "    for l in range(1, 11):\n",
    "        architecture = [\n",
    "            {\n",
    "                \"units\": l,\n",
    "                \"activation\": \"linear\"\n",
    "            }\n",
    "        ]\n",
    "        ann = ANNClassifier(architecture, learning_rate = 0.01, max_iter = 10)\n",
    "        df = pd.read_csv(\"iris.csv\")\n",
    "        X = df.values[:,:2].astype(float)\n",
    "        X = (X - np.min(X))/(np.max(X)-np.min(X))\n",
    "        y = df.values[:,4].reshape(len(df), 1)\n",
    "        classes = np.unique(df.values[:,4])\n",
    "        yfinal=[]\n",
    "        for ya in df.values[:,4]:\n",
    "            yfinal.append([0,0,0])\n",
    "            yfinal[-1][list(classes).index(ya)]=1\n",
    "        \n",
    "        ann.train(X, np.array(yfinal))\n",
    "\n",
    "        # train the network\n",
    "        # ann.train(X, y)\n",
    "\n",
    "        y_hat = np.array([classes[list(ta).index(1)] for ta in ann.predict(X)]).reshape(len(df), 1)\n",
    "        mask_correct = (y_hat == y).T[0]\n",
    "        fig, ax = plt.subplots()\n",
    "        markers = [\"o\", \"^\", \"x\"]\n",
    "        for i, label in enumerate(np.unique(y_hat)):\n",
    "            indices_pred = (y_hat == label)[:,0]\n",
    "            i1 = indices_pred & mask_correct\n",
    "            i2 = indices_pred & ~mask_correct\n",
    "            ax.scatter(X[i1, 0], X[i1, 1], color=\"green\", marker=markers[i])\n",
    "            ax.scatter(X[i2, 0], X[i2, 1], color=\"red\", marker=markers[i])\n",
    "        plt.show()\n",
    "        \n",
    "visualizeLearningOnIris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeMNISTPredictions(ann, num_predictions = 100):\n",
    "    # %matplotlib inline\n",
    "    X = dfMNIST.values[:,:-1].astype(float)\n",
    "    offset = 60000\n",
    "    cols = 10\n",
    "    rows = int(np.ceil(num_predictions / cols))\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "    for index in range(num_predictions):\n",
    "        a = ax[int(np.floor(index / cols)), index % cols]\n",
    "        a.imshow(X[index].reshape(28, 28), cmap=\"gray_r\")\n",
    "        a.set_xticks([])\n",
    "        a.set_yticks([])\n",
    "        prediction = ann.predict([X[index]])[0]\n",
    "        a.set_title(\"Prediction: \" + str(prediction))\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "visualizeMNISTPredictions(ann)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
